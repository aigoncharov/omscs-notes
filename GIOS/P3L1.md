# Scheduling

## Scheduling options

- Assign tasks immediately
    - Simple (FCFS - First come first serve)
- Assign simple tasks first
    - Maximized throughput (SJF - Simple job first)
- Assign complex tasks first
    - Maximize resource utilization

## Scheduling overview

Scheduler chooses one task from the `ready queue` (implemented via `runqueue` data structure) and runs it on CPU.
Task can get into the ready queue after:
- IO req
- It was interrupted in favor of another task (time slice expired)
- It is a newly created task (fork)
- It was interrupted because of an interrupt

Scheduler dispatched threads on CPU:
- Context switch
- Enter user mode
- Set PC
- Go!

Scheduler policy/algorithm defines what task is going to be selected.
Scheduling algo and runqueue limit each other.

## Run-to-completion (no preemption)

FCFS:
- runqueue - FIFO queue

SJF:
- runqueue - ordered queue or tree
- throughput is the same, avg completions time and average wait time is better

## Preemptive scheduling

### SJF

We don't really know exec time. Use heuristics based on history:
- how long did a task run last time?
- how long did a task run last n times? (windowed average)

### Priority based

runqueue:
- Several queues for each priority level
- Or tree ordered by the priority

Risk - starvation (higher priority queue never gets empty). Solution - priority aging. priority = f(initial priority, time spent in the queue), gets higher with time.

Priority inversion - situation when a higher priority task tries to lock on a mutex that a lower priority task have locked before. Solution - boost a priority of the required mutex owner.

### Round Robin

FCFS, but tasks may yield (e.g. for I/O). Round Robin + interleaving = preempt any task after curtain amount of time (time slice).

Timeslice - maximum amount of time uninterrupted time given to a task (`time quantum`).
Pros:
- short tasks finish sooner
- more responsive
- lengthy io ops initiated sooner
Cons:
- overheads (interrupt, context switch, schedule)

CPU-bound tasks - larger time slices. IO-bound tasks - smaller time slices.
Options:
- one runqueue, check task type, assign different slice size
- two different queues that correspond to different time slices (policies)

### MLFQ

Mix priority-based with several queues with different time slices:
-> [ timeslice = 8ms ] ->      | P1 | most io intensive
-> [ timeslice = 16ms ] ->     | P2 | medium io intensive
-> [ timeslice = infinite ] -> | P3 | cpu intensive

We cannot identify the type of the task. Solution: place all new tasks in the P1 runqueue, push to the lower priority queue if a task uses the entire timeslice and does not yeild volantarily, and vice versa (multi-level feedback queue (MLFQ))
-> [ timeslice = 8ms ] ------> | P1 | most io intensive
                       |
|-----------------------
-> [ timeslice = 16ms ] -----> | P2 | medium io intensive
                       |
|-----------------------
-> [ timeslice = infinite ] -> | P3 | cpu intensive

## Linux schedulers

### O(1)

140 priorities. Task groups:
- real-time (0-99)
- timesharing (100-139) - all user proceses (default: 120, nice value API: -20 to + 19)

timeslice depends on priority: smallest for low priority, highest for high priority.
feedback based on sleep time: longer sleep - priority boost (-5), smaller sleep - priority lowered (-5).

runqueue:
- 2 arrays (140 els, el for each P) of task queues
- scheduler runs task from the `active` queue
- bitmask indicates what queues in the array have tasks
- if task does not exceed its time slice and yields, it is placed in the `active` queue
- if task exceeds its time slice, it is placed in the `expired` queue
- once `active` queue is empty, swap the arrays

### Completely-fair scheduler (CFS)

runqueue - red-black tree.
Algo:
- rate tasks based on their time spent on the CPU (`vruntime`)
- build a red-black tree
- always schedule the leftmost node (the task with the smallest `vruntime`)
- preempt the currently running task if it runs longer than the leftmost node
- periodically adjust `vruntime`. increase `vruntime` faster for lower priority tasks

select task - O 1
add task - O log N

## Multicore systems

- per-CPU runqueue + load balancer

If several memory nodes, CPU can access one node faster. Schedule tasks with the state in a specific memory node to the appropriate CPU (NUMA-aware scheduling).

Hyperthreading (simultaneous multithreading - SMT) - multiple hardware-supported execution contexts on a single CPU. Add a second set of registers. Fast context switching.
Co-schedule CPU-bound and memory-bound tasks. Determine the type via hardware counters (cache misses, IPC, power data).
CPI experiment shows that CPI is not a viable metric in real world as there is no large distribution of CPI between real-world programs. Schedulers should be aware of resource contention, not just load balancing. LLC usage could be a better choice than CPI.